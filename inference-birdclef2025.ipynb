{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Stage 5: Ensemble Inference and Submission\n",
    "\n",
    "This notebook implements the final stage of the BirdCLEF 2025 pipeline: running high-performance inference on the competition soundscapes. Key features include:\n",
    "\n",
    "1. **Model Ensembling**: Combines multiple architectures (Time-wise SED, Frequency-wise SED, and standard Classifiers) using a weighted ensemble for more robust predictions.\n",
    "2. **Streamlined Preprocessing**: Efficiently chunks incoming 3-minute test soundscapes into 5-second segments for continuous monitoring.\n",
    "3. **Post-Processing Smoothing**: Multiplies segment-level probabilities by the mean probability of the entire clip to filter out transient False Positives.\n",
    "4. **Submission Formatting**: Automatically generates the  in the required format for the Kaggle platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b20b12c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:01.757041Z",
     "iopub.status.busy": "2025-06-08T10:21:01.756693Z",
     "iopub.status.idle": "2025-06-08T10:21:21.874299Z",
     "shell.execute_reply": "2025-06-08T10:21:21.873203Z"
    },
    "papermill": {
     "duration": 20.125722,
     "end_time": "2025-06-08T10:21:21.876219",
     "exception": false,
     "start_time": "2025-06-08T10:21:01.750497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 2.47 s, total: 14.8 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# --- Install missing packages offline---\n",
    "try:\n",
    "    import timm\n",
    "except ModuleNotFoundError:\n",
    "    print('Installing timm...')\n",
    "    !pip install -q /kaggle/input/offline-packages/timm-1.0.15-py3-none-any.whl\n",
    "    \n",
    "# --- Core libraries ---\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# --- Data handling ---\n",
    "import pandas as pd\n",
    "\n",
    "# --- PyTorch and torchaudio ---\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Miscellaneous ---\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Custom tools ---\n",
    "import timm\n",
    "import time\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cae513a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:21.888246Z",
     "iopub.status.busy": "2025-06-08T10:21:21.887216Z",
     "iopub.status.idle": "2025-06-08T10:21:21.905606Z",
     "shell.execute_reply": "2025-06-08T10:21:21.904586Z"
    },
    "papermill": {
     "duration": 0.025986,
     "end_time": "2025-06-08T10:21:21.907421",
     "exception": false,
     "start_time": "2025-06-08T10:21:21.881435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    # General\n",
    "    seed: int = 315\n",
    "    debug: bool = False\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    num_workers: int = 4\n",
    "\n",
    "    # Data paths\n",
    "    data_path: str = '/kaggle/input/birdclef-2025/'\n",
    "    metadata_path: str = field(init=False)\n",
    "    taxonomy_path: str = field(init=False)\n",
    "    sample_submission_path: str = field(init=False)\n",
    "    test_soundscapes_path: str = field(init=False)\n",
    "    train_soundscapes_path: str = field(init=False)\n",
    "    train_data_path: str = field(init=False)\n",
    "\n",
    "    # Audio config\n",
    "    topDB: int = 80\n",
    "    FS: int = 32000\n",
    "    CHUNK_LENGTH: float = 5.0\n",
    "    N_FFT: int = 1024\n",
    "    HOP_LENGTH: int = 512\n",
    "    N_MELS: int = 128\n",
    "    FMIN: int = 50\n",
    "    FMAX: int = 14000\n",
    "    POWER: int = 2\n",
    "    SPEC_DTYPE: str = 'float16'\n",
    "    SPEC_FRAMES: int = field(init=False)\n",
    "    CHUNK_SAMPLES: int = field(init=False)\n",
    "\n",
    "    # Model\n",
    "    model_name: str = 'efficientnet_b0'\n",
    "    pretrained: bool = False\n",
    "    input_directory: str = '/kaggle/input/offline-packages'\n",
    "    input_model_filename: str = field(init=False)\n",
    "    output_model_filename: str = field(init=False)\n",
    "    num_classes: int = field(init=False)\n",
    "\n",
    "    # \ud83d\udd01 Ensemble model weights\n",
    "    timewise_weights_path: str = '/kaggle/input/effnet28/efficientnet_b0_sed.pth'\n",
    "    freqwise_weights_path: str = '/kaggle/input/effnet14/efficientnet_b0_sed.pth'\n",
    "    classifier_weights_path: str = '/kaggle/input/effnet33/efficientnet_b0_sed.pth'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.metadata_path = os.path.join(self.data_path, 'train.csv')\n",
    "        self.taxonomy_path = os.path.join(self.data_path, 'taxonomy.csv')\n",
    "        self.sample_submission_path = os.path.join(self.data_path, 'sample_submission.csv')\n",
    "        self.test_soundscapes_path = os.path.join(self.data_path, 'test_soundscapes')\n",
    "        self.train_soundscapes_path = os.path.join(self.data_path, 'train_soundscapes')\n",
    "        self.train_data_path = os.path.join(self.data_path, 'train_audio')\n",
    "        self.input_model_filename = f\"{self.model_name}_pretrained.pth\"\n",
    "        self.output_model_filename = f\"{self.model_name}_sed.pth\"\n",
    "        self.SPEC_FRAMES = (self.FS * self.CHUNK_LENGTH) // self.HOP_LENGTH + 1\n",
    "\n",
    "        taxonomy_df = pd.read_csv(self.taxonomy_path)\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.CHUNK_SAMPLES = int(self.FS * self.CHUNK_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce5741",
   "metadata": {
    "papermill": {
     "duration": 0.005558,
     "end_time": "2025-06-08T10:21:21.917339",
     "exception": false,
     "start_time": "2025-06-08T10:21:21.911781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564ba241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:21.930167Z",
     "iopub.status.busy": "2025-06-08T10:21:21.929179Z",
     "iopub.status.idle": "2025-06-08T10:21:21.936873Z",
     "shell.execute_reply": "2025-06-08T10:21:21.935795Z"
    },
    "papermill": {
     "duration": 0.015787,
     "end_time": "2025-06-08T10:21:21.939388",
     "exception": false,
     "start_time": "2025-06-08T10:21:21.923601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EfficientNetClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple EfficientNet-based audio classifier (no SED / attention).\n",
    "\n",
    "    This model:\n",
    "    - Uses a pretrained EfficientNet backbone (e.g., b0\u2013b3)\n",
    "    - Replaces the classifier head with a custom one\n",
    "    - Includes dropout and batch normalization\n",
    "    - Outputs clip-level logits (or optionally, probabilities)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cfg : object\n",
    "        Configuration object with fields:\n",
    "        - model_name: str (e.g., 'efficientnet_b0')\n",
    "        - pretrained: bool\n",
    "        - num_classes: int\n",
    "        - model_weights: Optional[str]\n",
    "        - device: str\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "\n",
    "        # Load base model\n",
    "        self.backbone = timm.create_model(cfg.model_name, pretrained=cfg.pretrained)\n",
    "\n",
    "        # Replace classifier head\n",
    "        in_features = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_features),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(in_features, cfg.num_classes),\n",
    "            # nn.Sigmoid()  # Uncomment for multi-label classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor [B, 3, Freq, Time] (e.g., log-mel spectrogram)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor:\n",
    "            Logits (or probabilities if sigmoid enabled) [B, num_classes]\n",
    "        \"\"\"\n",
    "        return self.backbone(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00263a3f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:21.951661Z",
     "iopub.status.busy": "2025-06-08T10:21:21.950958Z",
     "iopub.status.idle": "2025-06-08T10:21:21.959849Z",
     "shell.execute_reply": "2025-06-08T10:21:21.958599Z"
    },
    "papermill": {
     "duration": 0.016921,
     "end_time": "2025-06-08T10:21:21.961472",
     "exception": false,
     "start_time": "2025-06-08T10:21:21.944551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EfficientNetTimeSED(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet with a custom SED head for time-wise attention.\n",
    "    \n",
    "    This model:\n",
    "    - Uses a pretrained EfficientNet backbone\n",
    "    - Applies a frequency-wise attention mechanism\n",
    "    - Outputs class probabilities for multi-class classification\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    cfg : object\n",
    "        Configuration object (an instance of CFG)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store config and device\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "\n",
    "        # Create model with the correct architecture\n",
    "        self.backbone = timm.create_model(cfg.model_name, pretrained=cfg.pretrained)\n",
    "\n",
    "        # Remove classifier head, we will add our own\n",
    "        self.feature_dim = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()  # Remove classifier\n",
    "\n",
    "        # Time-wise attention block -> attention mechanism to emphasize important time regions (when did the animal spoke?).\n",
    "        self.att_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, None)),          # Averages across frequency, keeps time axis intact. \u2192 shape becomes [B, C, 1, T]\n",
    "            nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=1), # \u2192 pointwise attention weighting\n",
    "            nn.Sigmoid()   # \u2192 [0, 1] weights per time step\n",
    "        )\n",
    "\n",
    "        # Custom classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(self.feature_dim, cfg.num_classes, kernel_size=1),\n",
    "            nn.AdaptiveMaxPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape [B, 3, M, T], where:\n",
    "            - B = Batch size\n",
    "            - M = Mel bands (frequency bins)\n",
    "            - T = Time frames\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor:\n",
    "            Output tensor of shape [B, num_classes]\n",
    "        \"\"\"\n",
    "        features = self.backbone.forward_features(x)  # EfficientNet backbone feature map [B, C, F', T'] where F' is the compressed frequency axis and T' the time steps (how many chunks/segments of the input timeline)\n",
    "        # print(features.shape)\n",
    "        attn = self.att_block(features)  # Attention on time bands [B, C, 1, T']\n",
    "        # print(attn.shape)\n",
    "        features = features * attn       # Apply attention ( dims down irrelevant time steps and boosts relevant ones)\n",
    "        \n",
    "        out = self.classifier(features)  # Classify [B, num_classes]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380db567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:21.971699Z",
     "iopub.status.busy": "2025-06-08T10:21:21.971393Z",
     "iopub.status.idle": "2025-06-08T10:21:21.980533Z",
     "shell.execute_reply": "2025-06-08T10:21:21.979562Z"
    },
    "papermill": {
     "duration": 0.016239,
     "end_time": "2025-06-08T10:21:21.982166",
     "exception": false,
     "start_time": "2025-06-08T10:21:21.965927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EfficientNetFrequencySED(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNet with a custom SED head for frequency-wise attention.\n",
    "    \n",
    "    This model:\n",
    "    - Uses a pretrained EfficientNet backbone\n",
    "    - Applies a frequency-wise attention mechanism\n",
    "    - Outputs class probabilities for multi-class classification\n",
    "    \n",
    "    Arguments:\n",
    "    ----------\n",
    "    cfg : object\n",
    "        Configuration object (assumes it's an instance of CFG)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store config and device\n",
    "        self.cfg = cfg\n",
    "        self.device = torch.device(cfg.device)\n",
    "\n",
    "        # Create model with the correct architecture\n",
    "        self.backbone = timm.create_model(cfg.model_name, pretrained=cfg.pretrained)\n",
    "\n",
    "        # Remove classifier head, we will add our own\n",
    "        self.feature_dim = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()  # Remove classifier\n",
    "\n",
    "        # Frequency-wise attention block -> attention mechanism to emphasize important frequency regions.\n",
    "        self.att_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((None, 1)),          # Mean over frequency bands\n",
    "            nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Custom classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(self.feature_dim, cfg.num_classes, kernel_size=1),\n",
    "            nn.AdaptiveMaxPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape [B, 3, M, T], where:\n",
    "            - B = Batch size\n",
    "            - M = Mel bands (frequency bins)\n",
    "            - T = Time frames\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        torch.Tensor:\n",
    "            Output tensor of shape [B, num_classes]\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        features = self.backbone.forward_features(x)  # EfficientNet backbone [B, C, M', T']\n",
    "        attn = self.att_block(features)  # Attention on frequency bands [B, C, T', 1]\n",
    "        features = features * attn       # Apply attention\n",
    "        \n",
    "        out = self.classifier(features)  # Classify [B, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec9d93c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:21.993094Z",
     "iopub.status.busy": "2025-06-08T10:21:21.992748Z",
     "iopub.status.idle": "2025-06-08T10:21:22.000323Z",
     "shell.execute_reply": "2025-06-08T10:21:21.999232Z"
    },
    "papermill": {
     "duration": 0.015587,
     "end_time": "2025-06-08T10:21:22.001988",
     "exception": false,
     "start_time": "2025-06-08T10:21:21.986401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Algorithm: Logistic Ensemble\n",
    "# Aggregates predictions from multiple specialized models to reduce variance.\n",
    "class ModelEnsembler(torch.nn.Module):\n",
    "    def __init__(self, cfg, model_classes, weights_paths, weights=None):\n",
    "        super().__init__()\n",
    "        self.models = torch.nn.ModuleList()\n",
    "        self.device = cfg.device\n",
    "        for cls, path in zip(model_classes, weights_paths):\n",
    "            model = cls(cfg).to(self.device)\n",
    "            state_dict = torch.load(path, map_location=self.device, weights_only=True)\n",
    "            if \"model\" in state_dict:\n",
    "                state_dict = state_dict[\"model\"]\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            model.eval()\n",
    "            self.models.append(model)\n",
    "        self.weights = weights or [1.0 / len(self.models)] * len(self.models)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ensemble_logits = 0\n",
    "        for model, weight in zip(self.models, self.weights):\n",
    "            logits = model(x)\n",
    "            ensemble_logits += weight * logits\n",
    "        return ensemble_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d13893e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:22.011949Z",
     "iopub.status.busy": "2025-06-08T10:21:22.011642Z",
     "iopub.status.idle": "2025-06-08T10:21:22.027433Z",
     "shell.execute_reply": "2025-06-08T10:21:22.026373Z"
    },
    "papermill": {
     "duration": 0.022644,
     "end_time": "2025-06-08T10:21:22.029009",
     "exception": false,
     "start_time": "2025-06-08T10:21:22.006365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Algorithm: Real-Time Audio Chunking\n",
    "# Converts raw waveform into normalized log-mel spectrogram chunks on-the-fly.\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocesses audio for EfficientNet inference by generating log-mel spectrograms.\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    process_file(file_path: str) -> List[torch.Tensor]:\n",
    "        Processes an audio file and returns a list of spectrogram tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.sample_rate = cfg.FS\n",
    "        self.chunk_samples = cfg.CHUNK_SAMPLES\n",
    "\n",
    "        # Torchaudio transforms for spectrogram generation\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=cfg.FS,\n",
    "            n_fft=cfg.N_FFT,\n",
    "            hop_length=cfg.HOP_LENGTH,\n",
    "            n_mels=cfg.N_MELS,\n",
    "            f_min=cfg.FMIN,\n",
    "            f_max=cfg.FMAX,\n",
    "            power=cfg.POWER\n",
    "        )\n",
    "\n",
    "        self.db_transform = T.AmplitudeToDB(top_db=cfg.topDB)\n",
    "\n",
    "    def _load_waveform(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Load audio and convert to mono.\n",
    "        \"\"\"\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)  # convert to mono\n",
    "            \n",
    "        waveform = waveform[..., :cfg.FS * 60]  # Truncate to 60s max\n",
    "        return waveform\n",
    "\n",
    "    def _split_into_chunks(self, waveform: torch.Tensor) -> list:\n",
    "        \"\"\"\n",
    "        Splits the waveform into 5-second chunks, applying padding if necessary.\n",
    "        \"\"\"\n",
    "        total_samples = waveform.shape[-1]\n",
    "        chunks = []\n",
    "\n",
    "        for start in range(0, total_samples, self.chunk_samples):\n",
    "            end = start + self.chunk_samples\n",
    "            chunk = waveform[..., start:end]\n",
    "\n",
    "            # If chunk is smaller than required, pad it\n",
    "            if chunk.shape[-1] < self.chunk_samples:\n",
    "                pad_size = self.chunk_samples - chunk.shape[-1]\n",
    "                chunk = F.pad(chunk, (0, pad_size))\n",
    "                #chunk = self.cyclic_pad(chunk, self.chunk_samples)\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "        return chunks\n",
    "\n",
    "    def cyclic_pad(self, chunk: torch.Tensor, desired_len: int) -> torch.Tensor:\n",
    "        repeats = (desired_len // chunk.shape[-1]) + 1\n",
    "        return chunk.repeat(1, repeats)[..., :desired_len]\n",
    "\n",
    "    def _waveform_to_spec(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert waveform to a normalized log-mel spectrogram.\n",
    "        \"\"\"\n",
    "        mel = self.mel_transform(waveform)\n",
    "        log_mel = self.db_transform(mel)\n",
    "        norm_spec = (log_mel - log_mel.mean()) / (log_mel.std() + 1e-6)\n",
    "        return norm_spec.squeeze(0).cpu()  # [128, Time]\n",
    "\n",
    "    def process_file(self, file_path: str) -> list:\n",
    "        \"\"\"\n",
    "        Full pipeline to process an audio file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_path : str\n",
    "            Path to the audio file.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        list of torch.Tensor:\n",
    "            List of log-mel spectrograms for each 5-second chunk.\n",
    "        \"\"\"\n",
    "        if cfg.debug:\n",
    "            print(f\"[INFO] Processing file: {file_path}\")\n",
    "        waveform = self._load_waveform(file_path)\n",
    "        chunks = self._split_into_chunks(waveform)\n",
    "        \n",
    "        spectrograms = []\n",
    "        for idx, chunk in enumerate(tqdm(chunks, desc=\"Generating Spectrograms\", leave=False)):\n",
    "            spec = self._waveform_to_spec(chunk)\n",
    "            spec = spec.unsqueeze(0).repeat(3, 1, 1)  # [3, M, T]\n",
    "            spectrograms.append(spec)\n",
    "        \n",
    "        return spectrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1663dbe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:22.039806Z",
     "iopub.status.busy": "2025-06-08T10:21:22.039060Z",
     "iopub.status.idle": "2025-06-08T10:21:22.052014Z",
     "shell.execute_reply": "2025-06-08T10:21:22.051061Z"
    },
    "papermill": {
     "duration": 0.020055,
     "end_time": "2025-06-08T10:21:22.053685",
     "exception": false,
     "start_time": "2025-06-08T10:21:22.033630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Algorithm: Submission Orchestrator\n",
    "# Manages the end-to-end flow from .ogg files to the finalized row-wise CSV output.\n",
    "class InferencePipeline:\n",
    "    \"\"\"\n",
    "    Inference pipeline for EfficientNetSED.\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    run_inference(file_list: List[str]) -> pd.DataFrame:\n",
    "        Runs inference on a list of audio files and returns predictions in DataFrame format.\n",
    "    \n",
    "    generate_submission(predictions: pd.DataFrame, sample_submission: str, output_path: str):\n",
    "        Generates the final submission CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocessor, cfg):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.cfg = cfg\n",
    "        self.device = cfg.device\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load label map\n",
    "        taxonomy = pd.read_csv(self.cfg.taxonomy_path)\n",
    "        self.label_map = taxonomy[\"primary_label\"].values\n",
    "\n",
    "    def run_inference(self, file_list):\n",
    "        \"\"\"\n",
    "        Run inference on a list of audio files and collect predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        file_list : List[str]\n",
    "            List of file paths for inference.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame:\n",
    "            DataFrame with row_id and species probabilities for submission.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Starting inference on soundscapes...\")\n",
    "        all_results = []\n",
    "\n",
    "        for file_path in tqdm(file_list, desc=\"Running Inference\"):\n",
    "            soundscape_id = os.path.basename(file_path).replace(\".ogg\", \"\")\n",
    "            if cfg.debug:\n",
    "                print(f\"[INFO] Processing {soundscape_id}\")\n",
    "            \n",
    "            # Process audio and get spectrogram chunks\n",
    "            spectrograms = self.preprocessor.process_file(file_path)\n",
    "            chunk_predictions = []\n",
    "            \n",
    "            for spec in spectrograms:\n",
    "                # Add batch dimension and move to device\n",
    "                spec = spec.unsqueeze(0).to(self.device)  # [1, 3, 128, 253]\n",
    "\n",
    "                # Inference\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(spec)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
    "\n",
    "                chunk_predictions.append(probs)\n",
    "\n",
    "            # Convert to array\n",
    "            chunk_predictions = np.stack(chunk_predictions)  # shape: [12, num_classes]\n",
    "    \n",
    "            # === Post-Processing: Multiply each chunk prob with mean prob per class ===\n",
    "            class_mean = chunk_predictions.mean(axis=0)  # [num_classes]\n",
    "            smoothed = chunk_predictions * class_mean  # shape: [12, num_classes]\n",
    "    \n",
    "            # Time tracking\n",
    "            current_time = 5\n",
    "            # Convert to DataFrame-like format\n",
    "            for i, probs in enumerate(smoothed):\n",
    "    \n",
    "                # Create row_id for submission format\n",
    "                row_id = f\"{soundscape_id}_{current_time}\"\n",
    "                    \n",
    "                # Append result\n",
    "                all_results.append([row_id] + list(probs))\n",
    "                    \n",
    "                # Move to the next 5-second window\n",
    "                current_time += 5\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        columns = [\"row_id\"] + list(self.label_map)\n",
    "        predictions_df = pd.DataFrame(all_results, columns=columns)\n",
    "        \n",
    "        print(f\"[INFO] Inference complete! Processed {len(file_list)} files.\")\n",
    "        return predictions_df\n",
    "\n",
    "    def generate_submission(self, predictions, sample_submission, species_ids, output_path=\"submission.csv\"):\n",
    "        \"\"\"\n",
    "        Generate submission CSV from predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : pd.DataFrame\n",
    "            DataFrame with the inference results.\n",
    "        \n",
    "        sample_submission : str\n",
    "            Path to the sample submission for formatting reference.\n",
    "        \n",
    "        output_path : str\n",
    "            Output path for the final submission file.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Creating submission dataframe...\")\n",
    "        \n",
    "        # Load sample submission to get the correct structure\n",
    "        sample_df = pd.read_csv(sample_submission, index_col='row_id')\n",
    "        \n",
    "        # Create a dictionary for DataFrame construction\n",
    "        row_ids = predictions['row_id']\n",
    "        submission_dict = {'row_id': row_ids}\n",
    "        \n",
    "        # Loop through each species and add the prediction to the dictionary\n",
    "        for species in species_ids:\n",
    "            if species in predictions.columns:\n",
    "                submission_dict[species] = predictions[species]\n",
    "            else:\n",
    "                submission_dict[species] = 0.0\n",
    "        \n",
    "        # Build the DataFrame\n",
    "        submission_df = pd.DataFrame(submission_dict)\n",
    "        submission_df.set_index('row_id', inplace=True)\n",
    "        \n",
    "        # Ensure column order matches\n",
    "        submission_df = submission_df[sample_df.columns]\n",
    "        submission_df = submission_df.reset_index()\n",
    "        \n",
    "        # Save to CSV\n",
    "        submission_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"[INFO] Submission saved to {output_path}\")\n",
    "        return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf080b6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:22.063876Z",
     "iopub.status.busy": "2025-06-08T10:21:22.062996Z",
     "iopub.status.idle": "2025-06-08T10:21:22.067596Z",
     "shell.execute_reply": "2025-06-08T10:21:22.066783Z"
    },
    "papermill": {
     "duration": 0.011712,
     "end_time": "2025-06-08T10:21:22.069648",
     "exception": false,
     "start_time": "2025-06-08T10:21:22.057936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#model = EfficientNetClassifier(cfg)\n",
    "#print(f\"[INFO] Loading weights from {cfg.custom_weights_path}\")\n",
    "#model.load_state_dict(torch.load(cfg.custom_weights_path, map_location=cfg.device, weights_only=True))\n",
    "#model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcbe858b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-08T10:21:22.079698Z",
     "iopub.status.busy": "2025-06-08T10:21:22.079387Z",
     "iopub.status.idle": "2025-06-08T10:21:23.736076Z",
     "shell.execute_reply": "2025-06-08T10:21:23.734848Z"
    },
    "papermill": {
     "duration": 1.663645,
     "end_time": "2025-06-08T10:21:23.737738",
     "exception": false,
     "start_time": "2025-06-08T10:21:22.074093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing ensemble model...\n",
      " \u2192 Model 0: EfficientNetFrequencySED\n",
      "    - Weights path: /kaggle/input/effnet14/efficientnet_b0_sed.pth\n",
      "    - Weight in ensemble: 0.5\n",
      " \u2192 Model 1: EfficientNetTimeSED\n",
      "    - Weights path: /kaggle/input/effnet28/efficientnet_b0_sed.pth\n",
      "    - Weight in ensemble: 0.3\n",
      " \u2192 Model 2: EfficientNetClassifier\n",
      "    - Weights path: /kaggle/input/effnet33/efficientnet_b0_sed.pth\n",
      "    - Weight in ensemble: 0.2\n",
      "[INFO] Ensemble model initialized with 3 models.\n",
      "\n",
      "[INFO] Found 0 test files for inference.\n",
      "\n",
      "[INFO] Starting inference on soundscapes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54adea23794b46c2a1c289f1ddaf52a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Inference: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Inference complete! Processed 0 files.\n",
      "[INFO] Creating submission dataframe...\n",
      "[INFO] Submission saved to submission.csv\n",
      "\n",
      "[INFO] Sample of final submission:\n",
      "Empty DataFrame\n",
      "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 207 columns]\n"
     ]
    }
   ],
   "source": [
    "# === Initialize components ===\n",
    "cfg = CFG()\n",
    "preprocessor = AudioPreprocessor(cfg)\n",
    "\n",
    "# === Define model classes and weights ===\n",
    "model_classes = [EfficientNetFrequencySED, EfficientNetTimeSED, EfficientNetClassifier]\n",
    "weights_paths = [cfg.freqwise_weights_path, cfg.timewise_weights_path, cfg.classifier_weights_path]\n",
    "ensemble_weights = [0.5, 0.3, 0.2]\n",
    "\n",
    "# === Initialize ensemble model ===\n",
    "print(\"[INFO] Initializing ensemble model...\")\n",
    "for i, (cls, path, wt) in enumerate(zip(model_classes, weights_paths, ensemble_weights)):\n",
    "    print(f\" \u2192 Model {i}: {cls.__name__}\")\n",
    "    print(f\"    - Weights path: {path}\")\n",
    "    print(f\"    - Weight in ensemble: {wt}\")\n",
    "\n",
    "ensemble_model = ModelEnsembler(cfg, model_classes, weights_paths, weights=ensemble_weights)\n",
    "print(f\"[INFO] Ensemble model initialized with {len(ensemble_model.models)} models.\\n\")\n",
    "\n",
    "# === Initialize inference pipeline ===\n",
    "inference_pipeline = InferencePipeline(ensemble_model, preprocessor, cfg)\n",
    "\n",
    "# === Load test audio files ===\n",
    "test_path = cfg.test_soundscapes_path  # Use train_soundscapes_path for debug\n",
    "test_files = [os.path.join(test_path, f) for f in sorted(os.listdir(test_path)) if f.endswith(\".ogg\")]\n",
    "print(f\"[INFO] Found {len(test_files)} test files for inference.\\n\")\n",
    "\n",
    "# === Run inference ===\n",
    "predictions = inference_pipeline.run_inference(test_files)\n",
    "\n",
    "# === Generate submission ===\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_path)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "\n",
    "submission_df = inference_pipeline.generate_submission(\n",
    "    predictions,\n",
    "    sample_submission=cfg.sample_submission_path,\n",
    "    species_ids=species_ids,\n",
    "    output_path=\"submission.csv\"\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Sample of final submission:\")\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f3713",
   "metadata": {
    "papermill": {
     "duration": 0.004964,
     "end_time": "2025-06-08T10:21:23.747955",
     "exception": false,
     "start_time": "2025-06-08T10:21:23.742991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7376846,
     "sourceId": 11750534,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7381770,
     "sourceId": 11758698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7401473,
     "sourceId": 11787912,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7402184,
     "sourceId": 11788907,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7408889,
     "sourceId": 11798172,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7420097,
     "sourceId": 11813746,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7420709,
     "sourceId": 11814612,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7421290,
     "sourceId": 11815474,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7428438,
     "sourceId": 11825275,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7438132,
     "sourceId": 11838842,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7447422,
     "sourceId": 11852188,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7447630,
     "sourceId": 11852484,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7451615,
     "sourceId": 11858863,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7452692,
     "sourceId": 11860333,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7453321,
     "sourceId": 11861222,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7460561,
     "sourceId": 11871616,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7463065,
     "sourceId": 11875127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7480710,
     "sourceId": 11900261,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7481255,
     "sourceId": 11901127,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7504387,
     "sourceId": 11936304,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7511766,
     "sourceId": 11948504,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7520211,
     "sourceId": 11959969,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7554672,
     "sourceId": 12008475,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7555125,
     "sourceId": 12009102,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7555684,
     "sourceId": 12009896,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7561424,
     "sourceId": 12018555,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7567800,
     "sourceId": 12028153,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7568419,
     "sourceId": 12029010,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7568742,
     "sourceId": 12029455,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7592759,
     "sourceId": 12062978,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7592967,
     "sourceId": 12063284,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7593055,
     "sourceId": 12063430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7594128,
     "sourceId": 12065091,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7600260,
     "sourceId": 12073858,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7600679,
     "sourceId": 12074637,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 239793644,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.769557,
   "end_time": "2025-06-08T10:21:26.810846",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-08T10:20:56.041289",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f25afe5591f4ffe9ff362533e89e583": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4d5256915c584ea884ef35588e67616f",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fee200641b294635993f823d536f272e",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0
      }
     },
     "403f7f353ecc42289288ba8604cc2df8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "47ee9eac28024a5eb34c9b7be5ccc967": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f313547f579a4fb6a5355090727ac711",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_fe006efa4168449c90e08444a0d5b740",
       "tabbable": null,
       "tooltip": null,
       "value": "Running\u2007Inference:\u2007"
      }
     },
     "4d5256915c584ea884ef35588e67616f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "54adea23794b46c2a1c289f1ddaf52a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_47ee9eac28024a5eb34c9b7be5ccc967",
        "IPY_MODEL_0f25afe5591f4ffe9ff362533e89e583",
        "IPY_MODEL_ae6741b8926d47daa1fd2a64d37a09ba"
       ],
       "layout": "IPY_MODEL_403f7f353ecc42289288ba8604cc2df8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "99dce603030045dab749e81bd6f9b04e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae6741b8926d47daa1fd2a64d37a09ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eb97018d9b8f4e3e8b951a206e03df58",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_99dce603030045dab749e81bd6f9b04e",
       "tabbable": null,
       "tooltip": null,
       "value": "\u20070/0\u2007[00:00&lt;?,\u2007?it/s]"
      }
     },
     "eb97018d9b8f4e3e8b951a206e03df58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f313547f579a4fb6a5355090727ac711": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe006efa4168449c90e08444a0d5b740": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fee200641b294635993f823d536f272e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}